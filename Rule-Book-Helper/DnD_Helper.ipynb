{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97bbff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dsu\\desktop\\tanner_allen_rag\\dnd-llm-env\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419ae36",
   "metadata": {},
   "source": [
    "installation of beautifulsoup to aid with url requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f212f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9acb95b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 PDFs from RuleBooks\n"
     ]
    }
   ],
   "source": [
    "# Folder containing all your rulebooks\n",
    "rulebooks_folder = \"RuleBooks\"\n",
    "\n",
    "# Automatically find all PDFs in the folder\n",
    "pdf_files = [os.path.join(rulebooks_folder, f) for f in os.listdir(rulebooks_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "# Read the PDFs into text\n",
    "documents = []\n",
    "for pdf_path in pdf_files:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    documents.append(text)\n",
    "\n",
    "print(f\"Loaded {len(documents)} PDFs from {rulebooks_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f6cdf",
   "metadata": {},
   "source": [
    "carry over pdf file locations and extract the text after converting them to useable text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aaf77444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 2148 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Embed the documents\n",
    "chunk_size = 200  # tokens/words per chunk\n",
    "all_chunks = []\n",
    "doc_index = []\n",
    "\n",
    "for doc_id, doc in enumerate(documents):\n",
    "    words = doc.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        all_chunks.append(chunk)\n",
    "        doc_index.append(doc_id)\n",
    "\n",
    "# Load embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # small but effective\n",
    "\n",
    "# Compute embeddings\n",
    "doc_embeddings = embed_model.encode(all_chunks, convert_to_numpy=True)\n",
    "print(f\"Created embeddings for {len(all_chunks)} chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d15af",
   "metadata": {},
   "source": [
    "embed read documents and chunk them for RAG use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "81219a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 2148 vectors\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index for L2 similarity search\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "print(f\"FAISS index contains {index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04eac9",
   "metadata": {},
   "source": [
    "faiss indexer for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e67c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Simple pipeline for text generation\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1)  # CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4d5a1",
   "metadata": {},
   "source": [
    "gpt-neo-125M for cpu based model since gpu is not available at the moment, this can be swapped out if gpu is available, tokenizers as well as a pipeline for text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "754c3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_docs(\n",
    "    query,\n",
    "    top_k=5,\n",
    "    max_tokens=200,\n",
    "    max_context_tokens=900,\n",
    "    sub_chunk_size=120\n",
    "):\n",
    "    # 1. Embed the query\n",
    "    query_vector = embed_model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # 2. Retrieve top-k chunks from FAISS\n",
    "    D, I = index.search(query_vector, top_k)\n",
    "\n",
    "    # 3. Build context (ONE sub-chunk per FAISS result)\n",
    "    context_chunks = []\n",
    "    token_count = 0\n",
    "\n",
    "    for idx in I[0]:\n",
    "        chunk = all_chunks[idx]\n",
    "        words = chunk.split()\n",
    "\n",
    "        # take ONLY the first sub-chunk\n",
    "        sub_chunk = \" \".join(words[:sub_chunk_size])\n",
    "        sub_tokens = len(sub_chunk.split())\n",
    "\n",
    "        if token_count + sub_tokens > max_context_tokens:\n",
    "            break\n",
    "\n",
    "        context_chunks.append(sub_chunk)\n",
    "        token_count += sub_tokens\n",
    "\n",
    "    retrieved_text = \"\\n\\n\".join(context_chunks)\n",
    "\n",
    "    # 4. Prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a knowledgeable Dungeons & Dragons assistant.\n",
    "Use the following context from official and supplementary D&D rulebooks to answer the user's question concisely and without repeating yourself.\n",
    "\n",
    "Context:\n",
    "{retrieved_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    # 5. Generate\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        repetition_penalty=1.5,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return result[0][\"generated_text\"][len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e443d00",
   "metadata": {},
   "source": [
    "RAG layout to feed query and interate through chunks to update query for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d7c0c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_webpage_text(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Fandom main article content\n",
    "    content = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    paragraphs = content.find_all([\"p\", \"li\"])\n",
    "    text = \"\\n\".join(p.get_text(\" \", strip=True) for p in paragraphs)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725c39",
   "metadata": {},
   "source": [
    "load text from url to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4c5cb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_web_source_to_index(url, chunk_size=120):\n",
    "    print(f\"Fetching: {url}\")\n",
    "\n",
    "    # 1. Load webpage text\n",
    "    text = load_webpage_text(url)\n",
    "\n",
    "    # ✅ SAFETY CHECK GOES HERE\n",
    "    if not text or len(text) < 500:\n",
    "        print(\"❌ Page too small or empty, skipping\")\n",
    "        return\n",
    "\n",
    "    # 2. Chunk text\n",
    "    words = text.split()\n",
    "    new_chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        new_chunks.append(chunk)\n",
    "\n",
    "    # 3. Embed new chunks\n",
    "    new_embeddings = embed_model.encode(\n",
    "        new_chunks,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "    # 4. Add to FAISS + chunk store\n",
    "    index.add(new_embeddings)\n",
    "    all_chunks.extend(new_chunks)\n",
    "\n",
    "    print(f\"✅ Added {len(new_chunks)} chunks from {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582c146",
   "metadata": {},
   "source": [
    "add text from webpage as new embeds and chunk them for faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5c629e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://forgottenrealms.fandom.com/wiki/Owlbear\n",
      "✅ Added 92 chunks from https://forgottenrealms.fandom.com/wiki/Owlbear\n",
      "Fetching: https://en.wikipedia.org/wiki/Owlbear\n",
      "✅ Added 44 chunks from https://en.wikipedia.org/wiki/Owlbear\n"
     ]
    }
   ],
   "source": [
    "add_web_source_to_index(\n",
    "    \"https://forgottenrealms.fandom.com/wiki/Owlbear\"\n",
    ")\n",
    "\n",
    "add_web_source_to_index(\n",
    "    \"https://en.wikipedia.org/wiki/Owlbear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590ed01",
   "metadata": {},
   "source": [
    "webpage insertions to add new webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e6cf3ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " No.\n",
      "\n",
      "Question:\n",
      "What is the most dangerous place to kill a level 5 player?\n",
      "\n",
      "Answer:\n",
      "The most dangerous place to kill an owl is in a forest. [ 6 ] [ 4 ] [ 3 ] [ 2 ] [ 20 ] [ 2 ] [ 3 ] [ 2 ] [ 1 ] [ 2 ] [ 1 ] [ 2 ] [ 2 ] [ 1 ] [ 1 ] [ 2 ] [ 1 ] [ 1 ] [ 2 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1 ] [ 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Test\n",
    "query = \"How dangerous is an owlbear for a level 5 party?\"\n",
    "answer = ask_with_docs(query)\n",
    "print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3d49b",
   "metadata": {},
   "source": [
    "test queries and print out of answers to queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnd-llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
